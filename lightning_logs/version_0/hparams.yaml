adam_epsilon: 1.0e-08
data_dir: datasets/
early_stop_callback: false
eval_batch_size: 8
fp_16: false
gradient_accumulation_steps: 1
learning_rate: 0.0003
max_grad_norm: 1.0
max_seq_length: 512
model_name_or_path: t5-base
n_gpu: 0
num_train_epochs: 1
opt_level: O1
output_dir: outputs/
seed: 42
tokenizer_name_or_path: t5-base
train_batch_size: 8
warmup_steps: 0
weight_decay: 0.0

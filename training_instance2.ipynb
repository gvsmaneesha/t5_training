{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGAUrV0Rc5lF2vwl+BiEtO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvsmaneesha/t5_training/blob/main/training_instance2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qfKh0wnvV_0",
        "outputId": "93a8e564-6e30-405d-d6bf-0ac4dbb17a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd ~/../content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf t5_training\n",
        "!git clone https://github.com/gvsmaneesha/t5_training.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFQ1qCHtviRS",
        "outputId": "71b7a97a-c241-4e63-ced9-f9d047d54d66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 't5_training'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 117 (delta 49), reused 76 (delta 24), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (117/117), 5.68 MiB | 8.87 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd t5_training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNzcSHaHvj_v",
        "outputId": "387b9011-3b0e-4244-acc6-bdc4ed32683c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/t5_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swPkDdtZvo7u",
        "outputId": "a8cfc40a-5f73-440f-cda9-b9408e008ea0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5624\n",
            "drwxr-xr-x 2 root root    4096 Feb 22 10:33 \u001b[0m\u001b[01;34mconfig\u001b[0m/\n",
            "drwxr-xr-x 2 root root    4096 Feb 22 10:33 \u001b[01;34mdatasets\u001b[0m/\n",
            "drwxr-xr-x 3 root root    4096 Feb 22 10:33 \u001b[01;34mlearning_modules\u001b[0m/\n",
            "drwxr-xr-x 3 root root    4096 Feb 22 10:33 \u001b[01;34mlightning_logs\u001b[0m/\n",
            "-rw-r--r-- 1 root root    2072 Feb 22 10:33 main.py\n",
            "-rw-r--r-- 1 root root      47 Feb 22 10:33 README.md\n",
            "-rw-r--r-- 1 root root     102 Feb 22 10:33 requirements.txt\n",
            "-rw-r--r-- 1 root root 5641405 Feb 22 10:33 t5_qa_training_pytorch_span_extraction.ipynb\n",
            "-rw-r--r-- 1 root root   84049 Feb 22 10:33 Training_Instance.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.12.5\n",
        "!pip install git+git://github.com/williamFalcon/pytorch-lightning.git@master --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRIpIVNDvpX7",
        "outputId": "4c514b52-572b-4dce-9328-7803bdecf643"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.12.5 in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (0.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (4.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.5) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.5) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.5) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.5) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
            "Collecting git+git://github.com/williamFalcon/pytorch-lightning.git@master\n",
            "  Cloning git://github.com/williamFalcon/pytorch-lightning.git (to revision master) to /tmp/pip-req-build-217s4pw9\n",
            "  Running command git clone -q git://github.com/williamFalcon/pytorch-lightning.git /tmp/pip-req-build-217s4pw9\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/PyTorchLightning/lightning-tutorials\n",
            "   * branch            290fb466de1fcc2ac6025f74b56906592911e856 -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (21.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (2.8.0)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (0.3.2)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (1.21.5)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (2022.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.0.dev0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.0.dev0) (3.0.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (1.43.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (57.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (2.0.11)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (1.7.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.0.dev0) (21.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from learning_modules.data_utils import create_dataset\n",
        "train,test,val = create_dataset(\"tweet_dataset.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozwvfdJmvzH5",
        "outputId": "dbf8e90a-8818-48d0-a041-efccfdd33269"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data import\n",
            "Created dataset !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input\n",
        "for a,b,_ in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n",
        "    print(\"sentiment:\", a, \"tweet:\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DNGfylRwEG_",
        "outputId": "215f5523-2302-4309-a687-6fff3c1c020d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment: love tweet:  addict? Me? Okay I admit I need help  BUT... I have been writing hits big bro. I really really have *smile*\n",
            "sentiment: hate tweet: http://bit.ly/147HEU  - don`t judge me. tucking myself in tonight  mogwai on repeat\n",
            "sentiment: neutral tweet:  I should post some photos of my robots  ok - offline for 4 hours now. ttfn (oh, so IM...)\n",
            "sentiment: surprise tweet: Morrningg  just slept for 12 hours and now i have a headache D;\n",
            "sentiment: love tweet: You make me happy, whether you know it or not  <3\n",
            "sentiment: happiness tweet: http://slingalink.com/eVicE1 my daughters kindergarden  !\n",
            "sentiment: love tweet:  OMG, you have curly hair! Too cute!\n",
            "sentiment: fun tweet:  I`ll be thinking about how many goals United are going to knock past City. How `bout you?\n",
            "sentiment: neutral tweet:  in june. the 6th. a sat!\n",
            "sentiment: neutral tweet:  I think it`s going to be a LONNNGG Weekend, but not the 3 day kind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target (what we're trying to predict)\n",
        "for _,_,c in zip(train.sentiment.values[:10], train.text.values[:10], train.selected_text.values[:10]):\n",
        "    print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxVj5-X4wHO7",
        "outputId": "4d28f20e-2563-4aa6-9498-e90f5d4a82c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*smile\n",
            "don`t judge me. tucking myself in tonight  mogwai on repeat\n",
            "I should post some photos of my robots  ok - offline for 4 hours now. ttfn (oh, so IM...)\n",
            "now i have a headache\n",
            "You make me happy, wh\n",
            "http://slingalink.com/eVicE1 my daughters kindergarden  !\n",
            "Too cute!\n",
            "I`ll be thinking about how many goals United are going to knock past City. How `bout you?\n",
            "in june. the 6th. a sat!\n",
            "I think it`s going to be a LONNNGG Weekend, but not the 3 day kind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out the GPU we have access to\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjfElWhUwKPT",
        "outputId": "50469a19-4d52-47e2-e1d0-81e1350e7023"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for NaNs\n",
        "train.isna().sum().sum(), test.isna().sum().sum(), val.isna().sum().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj_E2a5qwT6m",
        "outputId": "4ca75078-976b-45d4-9adc-d1254b398eac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This is the standard format for T5 targets\n",
        "# More info in transformers docs: https://huggingface.co/transformers/model_doc/t5.html\n",
        "train['selected_text'] = train['selected_text'] + ' </s>'\n",
        "val['selected_text'] = val['selected_text'] + ' </s>'\n",
        "\n",
        "# Apply Q&A structure\n",
        "# From Appendix D in the T5 paper\n",
        "processed_input_train = (\"question: \" + train.sentiment + \" context: \" + train.text)\n",
        "processed_input_test = (\"question: \" + test.sentiment + \" context: \" + test.text)\n",
        "processed_input_val = (\"question: \" + val.sentiment + \" context: \" + val.text)\n",
        "\n",
        "# Save data as string separated by \\n (new line)\n",
        "processed_input_str_train = '\\n'.join(processed_input_train.values.tolist())\n",
        "processed_input_str_test = '\\n'.join(processed_input_test.values.tolist())\n",
        "selected_text_str_train = '\\n'.join(train['selected_text'].values.tolist())\n",
        "processed_input_str_val = '\\n'.join(processed_input_val.values.tolist())\n",
        "selected_text_str_val = '\\n'.join(val['selected_text'].values.tolist())"
      ],
      "metadata": {
        "id": "F_9X2hV6wO1L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_input_train.iloc[0], train['selected_text'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo9SCUzgwZoB",
        "outputId": "5f5bba45-b997-4954-c77c-17ef8d2df17e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('question: love context:  addict? Me? Okay I admit I need help  BUT... I have been writing hits big bro. I really really have *smile*',\n",
              " '*smile </s>')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_input_test.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "u4WEy7qkwb0a",
        "outputId": "ab0e0339-957c-4797-b613-006d3df71717"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"question: surprise context:  Yeah, mine said 'That`s a nice picture' & gave me the red x!  Hope you get it working soon!\""
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZTF-qYYxHff",
        "outputId": "076f7878-511a-479b-f9d1-95ad3b983a0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5624\n",
            "drwxr-xr-x 2 root root    4096 Feb 22 10:33 \u001b[0m\u001b[01;34mconfig\u001b[0m/\n",
            "drwxr-xr-x 2 root root    4096 Feb 22 10:33 \u001b[01;34mdatasets\u001b[0m/\n",
            "drwxr-xr-x 3 root root    4096 Feb 22 10:33 \u001b[01;34mlearning_modules\u001b[0m/\n",
            "drwxr-xr-x 3 root root    4096 Feb 22 10:33 \u001b[01;34mlightning_logs\u001b[0m/\n",
            "-rw-r--r-- 1 root root    2072 Feb 22 10:33 main.py\n",
            "-rw-r--r-- 1 root root      47 Feb 22 10:33 README.md\n",
            "-rw-r--r-- 1 root root     102 Feb 22 10:33 requirements.txt\n",
            "-rw-r--r-- 1 root root 5641405 Feb 22 10:33 t5_qa_training_pytorch_span_extraction.ipynb\n",
            "-rw-r--r-- 1 root root   84049 Feb 22 10:33 Training_Instance.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save source files\n",
        "\n",
        "with open('datasets/train.source', 'w') as f:\n",
        "    f.write(processed_input_str_train)\n",
        "    \n",
        "# Making dev similar in this case\n",
        "with open('datasets/test.source', 'w') as f:\n",
        "    f.write(processed_input_str_test)\n",
        "    \n",
        "with open('datasets/val.source', 'w') as f:\n",
        "    f.write(processed_input_str_val)"
      ],
      "metadata": {
        "id": "Mcz0nKaRw39m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuRGMQQ1yC4n",
        "outputId": "e5712d41-db6e-4c18-ce0f-9e1773f1e546"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mconfig\u001b[0m/            README.md\n",
            "\u001b[01;34mdatasets\u001b[0m/          requirements.txt\n",
            "\u001b[01;34mlearning_modules\u001b[0m/  t5_qa_training_pytorch_span_extraction.ipynb\n",
            "\u001b[01;34mlightning_logs\u001b[0m/    Training_Instance.ipynb\n",
            "main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head datasets/train.source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HomjVaaCwqq5",
        "outputId": "fc104859-55f0-4832-ee62-2bf1cac9f285"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: love context:  addict? Me? Okay I admit I need help  BUT... I have been writing hits big bro. I really really have *smile*\n",
            "question: hate context: http://bit.ly/147HEU  - don`t judge me. tucking myself in tonight  mogwai on repeat\n",
            "question: neutral context:  I should post some photos of my robots  ok - offline for 4 hours now. ttfn (oh, so IM...)\n",
            "question: surprise context: Morrningg  just slept for 12 hours and now i have a headache D;\n",
            "question: love context: You make me happy, whether you know it or not  <3\n",
            "question: happiness context: http://slingalink.com/eVicE1 my daughters kindergarden  !\n",
            "question: love context:  OMG, you have curly hair! Too cute!\n",
            "question: fun context:  I`ll be thinking about how many goals United are going to knock past City. How `bout you?\n",
            "question: neutral context:  in june. the 6th. a sat!\n",
            "question: neutral context:  I think it`s going to be a LONNNGG Weekend, but not the 3 day kind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head datasets/test.source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88NXr5EOydJa",
        "outputId": "680394f4-bf21-46fc-e9d3-ed6a8058a618"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: surprise context:  Yeah, mine said 'That`s a nice picture' & gave me the red x!  Hope you get it working soon!\n",
            "question: worry context:  lol. sounds like don`t call me babe  have done that too !! have you thought that over ?\n",
            "question: worry context: Be my Yoko Ono and follow me wherever I may go !\n",
            "question: love context:  it`s called a telephone. I know it`s new technology, but it`s the way forward!\n",
            "question: neutral context: Back in Spain\n",
            "question: neutral context:  haha, it`s okay to be different!\n",
            "question: neutral context: Shakedown Street!!!\n",
            "question: worry context:  I`m chopped liver.\n",
            "question: worry context: I`d rather sit on a bench with a friendly psychiatric patient than go a party with some 'cool' person\n",
            "question: worry context:  Yikes!  I hope she was okay.  I never her heard her say her tummy hurt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head datasets/val.source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3IXJurEyrvQ",
        "outputId": "c2ed1719-3a14-4e06-dc56-5f01892f06f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: neutral context: _Molecule where is Sackiroth\n",
            "question: love context: Just saw Star Trek...one word: BombTastic!!! Go see it if you haven`t already. P.S. I love Jon Cho!!! Haha\n",
            "question: happiness context: Good morning to the world. Hope everyone is ok\n",
            "question: relief context: Funtime was not a lot of fun!! But finally done with it\n",
            "question: neutral context:  actually, web works fine. tweetdeck keeps crashing  i`m in NJ today avoiding nascar\n",
            "question: love context: lol i <3 spongebob\n",
            "question: happiness context: starting off my day with a little Northern Light\n",
            "question: neutral context: Almost dere  really tired eyes r really dry\n",
            "question: worry context: I chopped a fringe in last night got tired of having it hang in my eyes. Made a mess\n",
            "question: worry context:  and yes, I have seen your comment and yes, I did have the right answer.  I`ll get over it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('datasets/train.target', 'w') as f:\n",
        "    f.write(selected_text_str_train)\n",
        "    \n",
        "with open('datasets/val.target', 'w') as f:\n",
        "    f.write(selected_text_str_val)"
      ],
      "metadata": {
        "id": "Hp_guOM_yx8F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head datasets/train.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrfX0SWlzCu5",
        "outputId": "0324ee6e-0718-47e3-decd-4aa340af7923"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*smile </s>\n",
            "don`t judge me. tucking myself in tonight  mogwai on repeat </s>\n",
            "I should post some photos of my robots  ok - offline for 4 hours now. ttfn (oh, so IM...) </s>\n",
            "now i have a headache </s>\n",
            "You make me happy, wh </s>\n",
            "http://slingalink.com/eVicE1 my daughters kindergarden  ! </s>\n",
            "Too cute! </s>\n",
            "I`ll be thinking about how many goals United are going to knock past City. How `bout you? </s>\n",
            "in june. the 6th. a sat! </s>\n",
            "I think it`s going to be a LONNNGG Weekend, but not the 3 day kind. </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head datasets/val.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxVMuXCezHNz",
        "outputId": "61f95f0f-cb6a-4bcb-a98d-1764e9dc588b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_Molecule where is Sackiroth </s>\n",
            "love </s>\n",
            "Good </s>\n",
            "fun!! </s>\n",
            "actually, web works fine. tweetdeck keeps crashing  i`m in NJ today avoiding nascar </s>\n",
            "lol i <3 spongebob </s>\n",
            "starting off my day with a little Northern Light </s>\n",
            "tired </s>\n",
            "Made a mess </s>\n",
            "and yes, I have seen your comment and yes, I did have the right answer.  I`ll get over it. </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_batch(\n",
        "    input_ids, pad_token_id, attention_mask=None,\n",
        "):\n",
        "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
        "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
        "    if attention_mask is None:\n",
        "        return input_ids[:, keep_column_mask]\n",
        "    else:\n",
        "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])"
      ],
      "metadata": {
        "id": "GbD2cwDC4185"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "1hLvRf3szKC4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ckhw_RGG5Bnj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bzJjYWOtJYLq"
      },
      "source": [
        "def encode_file(tokenizer, data_path, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n",
        "    \"\"\"\n",
        "    This function reads the text files that we prepared and returns them in tokenized form.\n",
        "\n",
        "    Actually tokenizer.batch_encode_plus returns these as a list of dictionaries where \n",
        "    each dictionary contains the word piece indices among other relevant inputs for training & inference\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    with open(data_path, \"r\") as f:\n",
        "        for text in f.readlines():\n",
        "            tokenized = tokenizer.batch_encode_plus(\n",
        "                [text], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors,\n",
        "            )\n",
        "            examples.append(tokenized)\n",
        "    return examples\n",
        "\n",
        "\n",
        "class T5Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    This is the T5 dataset that can read our train, test, and dev files separately\n",
        "\n",
        "    This was patterned after the SummarizationDataset from the `transformer` library's summarization example (compatible with T5)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        data_dir=\"../working/\",\n",
        "        type_path=\"train\",\n",
        "        max_source_length=1024,\n",
        "        max_target_length=56,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Store the tokenizer\n",
        "        self.tokenizer = tokenizer\n",
        "        self.type_path = type_path\n",
        "        # Read the source and target files for the type of file (train, test, or val)\n",
        "        self.source = encode_file(tokenizer, os.path.join(data_dir, type_path + \".source\"), max_source_length)\n",
        "        self.target = None\n",
        "        if self.type_path != \"test\":\n",
        "            self.target = encode_file(tokenizer, os.path.join(data_dir, type_path + \".target\"), max_target_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Return example as a dictionary containing source_ids, src_mask, and target_ids\n",
        "        source_ids = self.source[index][\"input_ids\"].squeeze() # (1024,)\n",
        "        # We need masks for transformers to:\n",
        "        # 1) ignore padding for both the encoder and decoder stages (src_mask)\n",
        "        # 2) ignore future tokens at the decoder stage\n",
        "        src_mask = self.source[index][\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.type_path == \"test\":\n",
        "            return {\"source_ids\": source_ids, \"source_mask\": src_mask}\n",
        "\n",
        "        target_ids = self.target[index][\"input_ids\"].squeeze() # (56, )\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids}\n",
        "\n",
        "    # Static methods, much like class methods, are methods that are bound to a class rather than its object.\n",
        "    # They do not require a class instance creation. So, they are not dependent on the state of the object.\n",
        "    # https://www.programiz.com/python-programming/methods/built-in/staticmethod\n",
        "    @staticmethod\n",
        "    def trim_seq2seq_batch(batch, pad_token_id, test=False):\n",
        "        # Remove columns that are populated exclusively by pad_token_id\n",
        "        # This ensures that each batch is padded only uptil the \"max sequence length\"\n",
        "        # https://github.com/huggingface/transformers/blob/1e51bb717c04ca4b01a05a7a548e6b550be38628/src/transformers/tokenization_utils.py\n",
        "        source_ids, source_mask = trim_batch(batch[\"source_ids\"], pad_token_id, attention_mask=batch[\"source_mask\"])\n",
        "        if test:\n",
        "            return source_ids, source_mask, None\n",
        "        y = trim_batch(batch[\"target_ids\"], pad_token_id)\n",
        "        return source_ids, source_mask, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        The tensors are stacked together as they are yielded.\n",
        "\n",
        "        Collate function is applied to the output of a DataLoader as it is yielded.\n",
        "        \"\"\"\n",
        "        input_ids = torch.stack([x[\"source_ids\"] for x in batch]) # BS x SL\n",
        "        masks = torch.stack([x[\"source_mask\"] for x in batch]) # BS x SL\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask = trim_batch(input_ids, pad_token_id, attention_mask=masks)\n",
        "        if self.type_path == \"test\":\n",
        "            return {\"source_ids\": source_ids, \"source_mask\": source_mask}\n",
        "\n",
        "        target_ids = torch.stack([x[\"target_ids\"] for x in batch]) # BS x SL\n",
        "        # Remove columns that are purely padding\n",
        "        y = trim_batch(target_ids, pad_token_id)\n",
        "        # Return dictionary containing tensors\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": source_mask, \"target_ids\": y}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def set_seed(args: argparse.Namespace):\n",
        "    \"\"\"\n",
        "    Set all the seeds to make results replicable\n",
        "    \"\"\"\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "\n",
        "class T5Module(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Base Transformer model that uses Pytorch Lightning as a PyTorch wrapper.\n",
        "\n",
        "    T5 specific methods are implemented in T5Trainer\n",
        "    \"\"\"\n",
        "    def __init__(self, hparams: argparse.Namespace, **config_kwargs):\n",
        "        \"Initialize a model.\"\n",
        "\n",
        "        super(T5Module, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n",
        "        # Read the config file of the T5 model (T5Config)\n",
        "        # AutoConfig allows you to read the configuration for a specified model (e.g. in this case, t5-base)\n",
        "        # Reference: https://huggingface.co/transformers/model_doc/auto.html#autoconfig\n",
        "        self.config = AutoConfig.from_pretrained(self.hparams.model_name_or_path)\n",
        "        # Read the tokenizer of the T5 model (T5Tokenizer)\n",
        "        # AutoTokenizer allows you to read the tokenizer for a specified model (e.g. in this case, t5-base)\n",
        "        # Reference: https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.hparams.model_name_or_path,\n",
        "            cache_dir=cache_dir,\n",
        "        )\n",
        "        # Read the model file for the pre-trained T5 model (T5ForConditionalGeneration)\n",
        "        # AutoModelWithLMHead allows you to read any of the language modelling models from the transformers library (e.g. in this case, t5-base)\n",
        "        # Automodels reference: https://huggingface.co/transformers/model_doc/auto.html#automodel\n",
        "        self.model = AutoModelWithLMHead.from_pretrained(\n",
        "            self.hparams.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path), # Checkpoint is a TF format\n",
        "            config=self.config,\n",
        "            cache_dir=cache_dir,\n",
        "        )\n",
        "\n",
        "        # Save dataset params\n",
        "        self.dataset_kwargs: dict = dict(\n",
        "            data_dir=self.hparams.data_dir,\n",
        "            max_source_length=self.hparams.max_source_length,\n",
        "            max_target_length=self.hparams.max_target_length,\n",
        "        )\n",
        "\n",
        "    # Forward function\n",
        "    # Defines the forward pass of the module\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids, # Indices of input sequence tokens in the vocabulary. \n",
        "        attention_mask=None, # Mask to avoid performing attention on padding token indices\n",
        "        decoder_input_ids=None, # T5 uses the pad_token_id as the starting token for decoder_input_ids generation.\n",
        "        lm_labels=None # Labels for computing the sequence classification/regression loss (see T5Model). Note: loss is returned when lm_label is provided.\n",
        "        ):\n",
        "        \"\"\"\n",
        "         loss (torch.FloatTensor of shape (1,), optional, returned when lm_label is provided\n",
        "        \"\"\"\n",
        "        # Details on how to use this in the Hugging Face T5 docs: https://huggingface.co/transformers/model_doc/t5.html\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            lm_labels=lm_labels,\n",
        "        )\n",
        "\n",
        "    # Data preparation\n",
        "\n",
        "    def get_dataloader(self, type_path: str, batch_size: int, shuffle: bool = False) -> DataLoader:\n",
        "        dataset = T5Dataset(self.tokenizer, type_path=type_path, **self.dataset_kwargs)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, shuffle=shuffle)\n",
        "        return dataloader\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        dataloader = self.get_dataloader(\"train\", batch_size=self.hparams.train_batch_size, shuffle=True)\n",
        "        t_total = (\n",
        "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "            // self.hparams.gradient_accumulation_steps\n",
        "            * float(self.hparams.num_train_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        return self.get_dataloader(\"val\", batch_size=self.hparams.eval_batch_size)\n",
        "\n",
        "    def test_dataloader(self) -> DataLoader:\n",
        "        return self.get_dataloader(\"test\", batch_size=self.hparams.eval_batch_size)\n",
        "\n",
        "    # Configure optimizers\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "        model = self.model\n",
        "        # Weight decay explanation:\n",
        "        # Weight decay will not be applied to \"bias\" and \"LayerNorm.weight\" parameters\n",
        "        # When training neural networks, it is common to use \"weight decay,\" where after each update,\n",
        "        # the weights are multiplied by a factor slightly less than 1.\n",
        "        # This prevents the weights from growing too large, and can be seen as gradient descent on a quadratic regularization term.\n",
        "        # https://metacademy.org/graphs/concepts/weight_decay_neural_networks\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "        # Group parameters to those that will and will not have weight decay applied\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        # Use AdamW as an optimizer\n",
        "        # Intro here: https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    # Forward pass and calculate loss per batch (step)\n",
        "\n",
        "    def _step(self, batch, return_text=False):\n",
        "        \"\"\"\n",
        "        Runs forward pass and calculates loss per batch. Applied for training_step, and validation_step\n",
        "        \"\"\"\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone()\n",
        "        # Change pad_token_id to -100\n",
        "        lm_labels[y[:, 1:] == pad_token_id] = -100\n",
        "        # Run forward pass and calculate loss\n",
        "        outputs = self(source_ids, attention_mask=source_mask, decoder_input_ids=y_ids, lm_labels=lm_labels,)\n",
        "        # Only get loss from the output since that's all we need to apply our optimizer\n",
        "        loss = outputs[0]\n",
        "        if return_text:\n",
        "            target_text = [self.tokenizer.decode(ids) for ids in y_ids]\n",
        "            return loss, target_text\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    # Step during training\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Runs forward pass, calculates loss, and returns loss (and logs) in a dict\n",
        "        \"\"\"\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        # Notice that each training step loss is recorded on tensorboard, which makes sense since we're tracking loss per batch\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    # Adjust weights based on calculated gradients and learning rate scheduler\n",
        "\n",
        "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
        "        \"\"\"\n",
        "        Adjust weights based on calculated gradients + learning rate scheduler, and refresh gradients\n",
        "        Reference for optimizer_step: https://pytorch-lightning.readthedocs.io/en/latest/optimizers.html\n",
        "        \"\"\"\n",
        "        if self.trainer.use_tpu:\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            # Adjust weights based on calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "        # Refresh gradients (to zero)\n",
        "        optimizer.zero_grad()\n",
        "        # Update the learning rate scheduler\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "    # Step during validation\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Runs forward pass, calculates loss, and returns loss in a dict\n",
        "        \"\"\"\n",
        "\n",
        "        # Return source and target text to calculate jaccard score only for validation\n",
        "        loss, target_text = self._step(batch, return_text=True)\n",
        "\n",
        "        preds = self.test_step(batch, batch_idx)\n",
        "        preds_text = preds[\"preds\"]\n",
        "        # Track jaccard score to get validation accuracy\n",
        "        jaccard_score = [jaccard(p, t) for p, t in zip(preds_text, target_text)]\n",
        "\n",
        "        return {\"val_loss\": loss, \"jaccard_score\": jaccard_score}\n",
        "\n",
        "    # Show loss after validation\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        \"\"\"\n",
        "        Calculate average loss for all the validation batches\n",
        "        \"\"\"\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        jaccard_scores = sum([x[\"jaccard_score\"] for x in outputs], [])\n",
        "        avg_jaccard_score = np.mean(jaccard_scores)\n",
        "        tensorboard_logs = {\"val_loss\": avg_loss, \"jaccard_score\": avg_jaccard_score}\n",
        "        return {\"avg_val_loss\": avg_loss, \"avg_jaccard_score\": avg_jaccard_score, \"log\": tensorboard_logs}\n",
        "\n",
        "    # Step during testing\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Runs forward pass on test set and returns calculated loss, predictions, and targets\n",
        "        Note: this assumes that your test set has targets (doesn't have for kaggle).\n",
        "        \"\"\"\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask, _ = T5Dataset.trim_seq2seq_batch(batch, pad_token_id, test=True)\n",
        "        # NOTE: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py\n",
        "        # Generate reference: https://github.com/huggingface/transformers/blob/3e0f06210646a440509efa718b30d18322d6a830/src/transformers/modeling_utils.py#L769\n",
        "        # For the sentiment span extraction task, turning off early stopping proved superior\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=source_ids,\n",
        "            attention_mask=source_mask,\n",
        "            num_beams=1,\n",
        "            max_length=80,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            use_cache=True,\n",
        "        )\n",
        "        preds = [\n",
        "            self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for g in generated_ids\n",
        "        ]\n",
        "\n",
        "        return {\"preds\": preds}\n",
        "\n",
        "    # Note: we don't attempt to print the loss from the test set, because it's assumed that we don't have the test targets\n",
        "    def test_end(self, outputs):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        preds = []\n",
        "        for pred in outputs:\n",
        "            preds += pred[\"preds\"]\n",
        "        return {\"preds\": preds}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        \"\"\"\n",
        "        Save test predictions and targets as text files and return the calculated loss for the test set\n",
        "        \"\"\"\n",
        "        output_test_predictions_file = os.path.join(self.hparams.output_dir, \"test_predictions.txt\")\n",
        "        # write predictions and targets for later rouge evaluation.\n",
        "        with open(output_test_predictions_file, \"w+\") as p_writer:\n",
        "            for output_batch in outputs:\n",
        "                p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\n",
        "            p_writer.close()\n",
        "\n",
        "        return self.test_end(outputs)\n",
        "\n",
        "    def get_tqdm_dict(self):\n",
        "        \"\"\"\n",
        "        Print average loss and learning rate at each step\n",
        "        \"\"\"\n",
        "        avg_loss = getattr(self.trainer, \"avg_loss\", 0.0)\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "        return tqdm_dict\n",
        "\n",
        "    def _feature_file(self, mode):\n",
        "        return os.path.join(\n",
        "            self.hparams.data_dir,\n",
        "            \"cached_{}_{}_{}\".format(\n",
        "                mode,\n",
        "                list(filter(None, self.hparams.model_name_or_path.split(\"/\"))).pop(),\n",
        "                str(self.hparams.max_seq_length),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def is_logger(self):\n",
        "        return self.trainer.proc_rank <= 0\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parser, root_dir):\n",
        "        parser.add_argument(\n",
        "            \"--model_name_or_path\",\n",
        "            default=None,\n",
        "            type=str,\n",
        "            required=True,\n",
        "            help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--tokenizer_name\",\n",
        "            default=\"\",\n",
        "            type=str,\n",
        "            help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--cache_dir\",\n",
        "            default=\"\",\n",
        "            type=str,\n",
        "            help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
        "        )\n",
        "        parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "        parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "        parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "        parser.add_argument(\n",
        "            \"--num_train_epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\"\n",
        "        )\n",
        "\n",
        "        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n",
        "        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n",
        "\n",
        "        parser.add_argument(\n",
        "            \"--max_source_length\",\n",
        "            default=1024,\n",
        "            type=int,\n",
        "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--max_target_length\",\n",
        "            default=56,\n",
        "            type=int,\n",
        "            help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\",\n",
        "        )\n",
        "\n",
        "        parser.add_argument(\n",
        "            \"--data_dir\",\n",
        "            default=None,\n",
        "            type=str,\n",
        "            required=True,\n",
        "            help=\"The input data dir. Should contain the dataset files for the text generation task.\",\n",
        "        )\n",
        "        return parser\n",
        "\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        logger.info(\"***** Validation results *****\")\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "            # Log results\n",
        "            for key in sorted(metrics):\n",
        "                if key not in [\"log\", \"progress_bar\"]:\n",
        "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "\n",
        "def add_generic_args(parser, root_dir):\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--n_gpu\", type=int, default=1)\n",
        "    parser.add_argument(\"--n_tpu_cores\", type=int, default=0)\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "\n",
        "def generic_train(model: T5Module, args: argparse.Namespace):\n",
        "    # init model\n",
        "    set_seed(args)\n",
        "\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "\n",
        "    # Can take out checkpoint saving after each epoch to save memory\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
        "    )\n",
        "\n",
        "    train_params = dict(\n",
        "        accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "        gpus=args.n_gpu,\n",
        "        max_epochs=args.num_train_epochs,\n",
        "        early_stop_callback=False,\n",
        "        gradient_clip_val=args.max_grad_norm,\n",
        "        checkpoint_callback=checkpoint_callback,\n",
        "        callbacks=[LoggingCallback()],\n",
        "    )\n",
        "\n",
        "    if args.fp16:\n",
        "        train_params[\"use_amp\"] = args.fp16\n",
        "        train_params[\"amp_level\"] = args.fp16_opt_level\n",
        "\n",
        "    if args.n_tpu_cores > 0:\n",
        "        global xm\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        train_params[\"num_tpu_cores\"] = args.n_tpu_cores\n",
        "        train_params[\"gpus\"] = 0\n",
        "\n",
        "    if args.n_gpu > 1:\n",
        "        train_params[\"distributed_backend\"] = \"ddp\"\n",
        "\n",
        "    trainer = pl.Trainer(**train_params)\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer.fit(model)\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "XD7sYNA05Kki"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # If output_dir not provided, a folder will be generated in pwd\n",
        "    if not args.output_dir:\n",
        "        args.output_dir = os.path.join(\"./results\", f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",)\n",
        "        os.makedirs(args.output_dir)\n",
        "    model = T5Module(args)\n",
        "    trainer = generic_train(model, args)\n",
        "\n",
        "    # Save the last model as model.bin\n",
        "    #checkpoints = list(sorted(glob.glob(os.path.join(args.output_dir, \"checkpointepoch=*.ckpt\"), recursive=True)))\n",
        "    #model = model.load_from_checkpoint(checkpoints[-1])\n",
        "    model.model.save_pretrained(args.output_dir)\n",
        "    # Save tokenizer files\n",
        "    model.tokenizer.save_pretrained('./')\n",
        "    \n",
        "    # Optionally, predict on dev set and write to output_dir\n",
        "    if args.do_predict:\n",
        "        # See https://github.com/huggingface/transformers/issues/3159\n",
        "        # pl use this format to create a checkpoint:\n",
        "        # https://github.com/PyTorchLightning/pytorch-lightning/blob/master\\\n",
        "        # /pytorch_lightning/callbacks/model_checkpoint.py#L169\n",
        "        trainer.test(model)\n",
        "    return trainer\n"
      ],
      "metadata": {
        "id": "QtOIClSv6JrA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir output"
      ],
      "metadata": {
        "id": "aUACqbBs6bbv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfgOXzhZ6eLy",
        "outputId": "624c6bf1-2d63-43b2-ff2f-ca74d77d9a6d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mconfig\u001b[0m/            \u001b[01;34moutput\u001b[0m/\n",
            "\u001b[01;34mdatasets\u001b[0m/          README.md\n",
            "\u001b[01;34mlearning_modules\u001b[0m/  requirements.txt\n",
            "\u001b[01;34mlightning_logs\u001b[0m/    t5_qa_training_pytorch_span_extraction.ipynb\n",
            "main.py            Training_Instance.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Will set gpu on as soon as at least 1 batch works on cpu\n",
        "# TODO: Consider factors here: https://github.com/huggingface/transformers/issues/3387\n",
        "# Change LR to 1e-3 to 1e-4\n",
        "# \n",
        "ARGS_STR = \"\"\"\n",
        "--data_dir=/content/ \\\n",
        "--model_name_or_path=t5-base \\\n",
        "--learning_rate=3e-5 \\\n",
        "--train_batch_size=32 \\\n",
        "--output_dir=/content/output/ \\\n",
        "--do_train \\\n",
        "--n_gpu=1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--max_source_length 80 \\\n",
        "\"\"\"\n",
        "#\n",
        "#--eval_batch_size=3 \\\n",
        "#--do_predict \\\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "add_generic_args(parser, os.getcwd())\n",
        "parser = T5Module.add_model_specific_args(parser, os.getcwd())\n",
        "args = parser.parse_args(ARGS_STR.split())\n",
        "trainer = main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "ISHSBPob6h-o",
        "outputId": "89289291-2e14-41c1-868e-cbff0b478b36"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-17b7825f5aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_model_specific_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARGS_STR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-eedcefc564a8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneric_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-5168264b93c6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hparams, **config_kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT5Module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mcache_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Read the config file of the T5 model (T5Config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1223\u001b[0m                     \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2nxO8kbz8xx2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
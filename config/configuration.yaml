args_dict:
    data_dir : datasets/
    output_dir : outputs/
    model_name_or_path : t5-base
    tokenizer_name_or_path : t5-base
    max_seq_length : 512
    learning_rate : 3e-4
    weight_decay : 0.0
    adam_epsilon : 1e-8
    warmup_steps : 0
    train_batch_size : 8
    eval_batch_size : 8
    num_train_epochs : 1
    gradient_accumulation_steps : 1
    n_gpu : 0
    early_stop_callback : False
    fp_16 : False
    opt_level : 'O1'
    max_grad_norm : 1.0
    seed : 42



train_params:
    accumulate_grad_batches : args.gradient_accumulation_steps,
    gpus : args.n_gpu,
    max_epochs : args.num_train_epochs,
    precision : 16 if args.fp_16 else 32,
    amp_level : args.opt_level,
    gradient_clip_val : args.max_grad_norm,
